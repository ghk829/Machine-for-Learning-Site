{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "def make_vectorize(doclist):\n",
    "    results={}\n",
    "    for j,i in enumerate(doclist):\n",
    "        results[j]=[w for w in tokenizer_porter(i.lower()) if w not in stop]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docu1 = '''There are two parts to this question. The first part is \"what is the form of function learned by these methods?\" For NN and SVM this is typically the same. For example, a single hidden layer neural network uses exactly the same form of model as an SVM. That is:\n",
    "\n",
    "Given an input vector x, the output is: output(x) = sum_over_all_i weight_i * nonlinear_function_i(x)\n",
    "\n",
    "Generally the nonlinear functions will also have some parameters. So these methods need to learn how many nonlinear functions should be used, what their parameters are, and what the value of all the weight_i weights should be.\n",
    "\n",
    "Therefore, the difference between a SVM and a NN is in how they decide what these parameters should be set to. Usually when someone says they are using a neural network they mean they are trying to find the parameters which minimize the mean squared prediction error with respect to a set of training examples. They will also almost always be using the stochastic gradient descent optimization algorithm to do this. SVM's on the other hand try to minimize both training error and some measure of \"hypothesis complexity\". So they will find a set of parameters that fits the data but also is \"simple\" in some sense. You can think of it like Occam's razor for machine learning. The most common optimization algorithm used with SVMs is sequential minimal optimization.\n",
    "\n",
    "Another big difference between the two methods is that stochastic gradient descent isn't guaranteed to find the optimal set of parameters when used the way NN implementations employ it. However, any decent SVM implementation is going to find the optimal set of parameters. People like to say that neural networks get stuck in a local minima while SVMs don't.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docu2 = '''\n",
    "31\n",
    "down vote\n",
    "accepted\n",
    "Standard Implementations of Neural Networks\n",
    "\n",
    "FANN is a very popular implementation in C/C++ and has bindings for many other languages.\n",
    "I think WEKA hasn't got a very good implementation for neural networks. There is a better library for Java (and C#): Encog.\n",
    "In scikit-learn (Python) 0.18 (current developement version) there will be an implementation of feed-forward neural networks (API documentation).\n",
    "PyBrain (Python) contains different types of neural networks and training methods.\n",
    "And I must mention my own project, which is called OpenANN (Documentation). It is written in C++ and has Python bindings.\n",
    "Deep Learning\n",
    "\n",
    "Because there is a huge hype around neural networks at the moment (known as \"deep learning\") there are many research libraries available that might possibly not be so easy to set up, integrate and use. On the other hand, they provide leading edge functionality and high performance (with GPUs etc.).\n",
    "\n",
    "Based on Theano (Python):\n",
    "\n",
    "Keras is the best of this category in my opinion: usable, powerful and actively developed. As an alternative to Theano it can use Google's Tensorflow as a backend.\n",
    "Blocks\n",
    "Lasagne\n",
    "Theanets\n",
    "scikit-neuralnetwork\n",
    "Pylearn 2 (not actively developed any more)\n",
    "Others:\n",
    "\n",
    "TensorFlow from Google (C++/Python)\n",
    "Caffe in C++ with Python bindings\n",
    "Neon provides very efficient implementations (Python)\n",
    "Neural Networks for Torch 7 (Lua, Torch 7 is a \"Matlab-like environment\", overview of machine learning algorithms in Torch)\n",
    "Deeplearning4j (Java)\n",
    "mxnet (C++, Python, R, Scala, Julia, Matlab, Javascript)\n",
    "Chainer (Python)\n",
    "MatConvNet (Matlab)\n",
    "PaddlePaddle, CUDA/C++ with Python bindings\n",
    "cuda-convnet2 in CUDA/C++ with Python bindings\n",
    "Hebel (Python), not actively developed any more\n",
    "A performance comparison for GPU-accelerated libraries can be found here.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doclist = [docu1,docu2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result=make_vectorize(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('SearchList.txt', 'r') as fp:\n",
    "    keys = fp.read().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_list(result,keys):\n",
    "    count_list={}\n",
    "    for j in range(len(result)):\n",
    "        some_dict ={}\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] in result[j]:\n",
    "                some_dict[keys[i]] =result[j].count(keys[i])\n",
    "        count_list[j] = some_dict\n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'algorithm': 2,\n",
       "  'data': 1,\n",
       "  'hidden': 1,\n",
       "  'local': 1,\n",
       "  'method': 2,\n",
       "  'model': 1,\n",
       "  'network': 3,\n",
       "  'neural': 3,\n",
       "  'vector': 1},\n",
       " 1: {'algorithm': 1,\n",
       "  'base': 1,\n",
       "  'deep': 1,\n",
       "  'network': 5,\n",
       "  'neural': 6,\n",
       "  'research': 1}}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_list(result,keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

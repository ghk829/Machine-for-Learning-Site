{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import docclass\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docu1 = '''There are two parts to this question. The first part is \"what is the form of function learned by these methods?\" For NN and SVM this is typically the same. For example, a single hidden layer neural network uses exactly the same form of model as an SVM. That is:\n",
    "\n",
    "Given an input vector x, the output is: output(x) = sum_over_all_i weight_i * nonlinear_function_i(x)\n",
    "\n",
    "Generally the nonlinear functions will also have some parameters. So these methods need to learn how many nonlinear functions should be used, what their parameters are, and what the value of all the weight_i weights should be.\n",
    "\n",
    "Therefore, the difference between a SVM and a NN is in how they decide what these parameters should be set to. Usually when someone says they are using a neural network they mean they are trying to find the parameters which minimize the mean squared prediction error with respect to a set of training examples. They will also almost always be using the stochastic gradient descent optimization algorithm to do this. SVM's on the other hand try to minimize both training error and some measure of \"hypothesis complexity\". So they will find a set of parameters that fits the data but also is \"simple\" in some sense. You can think of it like Occam's razor for machine learning. The most common optimization algorithm used with SVMs is sequential minimal optimization.\n",
    "\n",
    "Another big difference between the two methods is that stochastic gradient descent isn't guaranteed to find the optimal set of parameters when used the way NN implementations employ it. However, any decent SVM implementation is going to find the optimal set of parameters. People like to say that neural networks get stuck in a local minima while SVMs don't.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=nltk.tokenize.sent_tokenize(docu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl=docclass.fisherclassifier(docclass.getwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cl.setdb('test.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\kikun\\Desktop')\n",
    "with open('SearchList.txt', 'r') as fp:\n",
    "    keys = [i for i in fp.read().lower().split() if not i in stop]\n",
    "    keys=set(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('SearchList.txt', 'r') as fp:\n",
    "    keys2 = [i for i in fp.read().lower().split('\\n') if not i in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_have_it(s,key):\n",
    "    index=[]\n",
    "    for i in s:\n",
    "        if key in i:\n",
    "            index.append(key)\n",
    "        else: index.append(0)\n",
    "    return index\n",
    "def get_results_index(s,keys2):\n",
    "    results={}\n",
    "    for i in keys2:\n",
    "        if any(count_have_it(s,i)):\n",
    "            results[i]=count_have_it(s,i)\n",
    "    return results\n",
    "def get_number(s,cl,key):\n",
    "    num=0\n",
    "    for i in range(len(s)):\n",
    "        if cl.classify(s[i])==key:\n",
    "            num+=1\n",
    "    return num\n",
    "def train_bayes(cl,index):\n",
    "    for i in range(len(index)):\n",
    "        cl.train(s[i],index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexs=get_results_index(s,keys2)\n",
    "index=indexs['neural network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kikun\\Anaconda3\\lib\\docclass.py:14: FutureWarning: split() requires a non-empty pattern match.\n",
      "  words = [s.lower() for s in splitter.split(doc)\n"
     ]
    }
   ],
   "source": [
    "train_bayes(cl,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kikun\\Anaconda3\\lib\\docclass.py:14: FutureWarning: split() requires a non-empty pattern match.\n",
      "  words = [s.lower() for s in splitter.split(doc)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.get_number(s,'neural network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docu2 = '''\n",
    "31\n",
    "down vote\n",
    "accepted\n",
    "Standard Implementations of Neural Networks\n",
    "\n",
    "FANN is a very popular implementation in C/C++ and has bindings for many other languages.\n",
    "I think WEKA hasn't got a very good implementation for neural networks. There is a better library for Java (and C#): Encog.\n",
    "In scikit-learn (Python) 0.18 (current developement version) there will be an implementation of feed-forward neural networks (API documentation).\n",
    "PyBrain (Python) contains different types of neural networks and training methods.\n",
    "And I must mention my own project, which is called OpenANN (Documentation). It is written in C++ and has Python bindings.\n",
    "Deep Learning\n",
    "\n",
    "Because there is a huge hype around neural networks at the moment (known as \"deep learning\") there are many research libraries available that might possibly not be so easy to set up, integrate and use. On the other hand, they provide leading edge functionality and high performance (with GPUs etc.).\n",
    "\n",
    "Based on Theano (Python):\n",
    "\n",
    "Keras is the best of this category in my opinion: usable, powerful and actively developed. As an alternative to Theano it can use Google's Tensorflow as a backend.\n",
    "Blocks\n",
    "Lasagne\n",
    "Theanets\n",
    "scikit-neuralnetwork\n",
    "Pylearn 2 (not actively developed any more)\n",
    "Others:\n",
    "\n",
    "TensorFlow from Google (C++/Python)\n",
    "Caffe in C++ with Python bindings\n",
    "Neon provides very efficient implementations (Python)\n",
    "Neural Networks for Torch 7 (Lua, Torch 7 is a \"Matlab-like environment\", overview of machine learning algorithms in Torch)\n",
    "Deeplearning4j (Java)\n",
    "mxnet (C++, Python, R, Scala, Julia, Matlab, Javascript)\n",
    "Chainer (Python)\n",
    "MatConvNet (Matlab)\n",
    "PaddlePaddle, CUDA/C++ with Python bindings\n",
    "cuda-convnet2 in CUDA/C++ with Python bindings\n",
    "Hebel (Python), not actively developed any more\n",
    "A performance comparison for GPU-accelerated libraries can be found here.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
